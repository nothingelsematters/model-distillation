# Model Distillation

An ITMO University course home work assignment.

> На лекции мы упоминали Model Distillation в контексте DistillBert от HuggingFace. В этом задании вам предлагается применить дистилляцию к произвольной модели и попробовать добиться сокращения размера модели-ученика при почти полном сохранении качества по сравнению с моделью-учителем.
>
> Посмотрите следующее [руководство](https://habr.com/ru/company/avito/blog/485290/) и попробуйте применить описанные идеи к какой-либо модели. Не обязательно брать BERT, вполне подойдет BiLSTM или что-то подобное, для чего известны метрики на каком-либо датасете.
