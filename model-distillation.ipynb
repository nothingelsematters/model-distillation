{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "isolated-reducing",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "convenient-spyware",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional\n",
    "import os\n",
    "\n",
    "from teacher import CNN_Teacher\n",
    "from student import CNN_Student\n",
    "from common import MODEL_DIRECTORY, accuracy, evaluate, get_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "successful-prague",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def distillation_loss(logits_stu, logits_base, t):\n",
    "    logits_stu = logits_stu / t\n",
    "    logits_base = logits_base / t\n",
    "    pred_stu = functional.log_softmax(logits_stu,  dim=1)\n",
    "    prop_base = nn.Softmax(dim=1)(logits_base)\n",
    "    pred_base = torch.argmax(prop_base, dim=1)\n",
    "    return functional.nll_loss(pred_stu, pred_base, reduction='sum')\n",
    "    \n",
    "def train(model, iterator, optimizer, teacher, t, alpha):    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model = model.train()\n",
    "    teacher = teacher.eval()\n",
    "\n",
    "    for (x, y) in iterator:    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if teacher is not None:\n",
    "            y_pred, logits_pred = model(x)        \n",
    "            _, logits_teacher = teacher(x)\n",
    "\n",
    "            dist_loss = distillation_loss(logits_pred, logits_teacher, t)            \n",
    "            stu_loss = functional.nll_loss(y_pred, y, reduction='sum')     \n",
    "            loss = alpha * dist_loss + (1 - alpha) * stu_loss\n",
    "        else:\n",
    "            y_pred, _ = model(x)        \n",
    "            loss = functional.nll_loss(y_pred, y, reduction='sum')   \n",
    "\n",
    "        acc = accuracy(y_pred, y)        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29c78a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher = CNN_Teacher()\n",
    "teacher.load_state_dict(torch.load(os.path.join(MODEL_DIRECTORY, 'teacher.pt')))\n",
    "teacher = teacher.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rubber-storage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data: 50000 train and 10000 test examples\n",
      "Saving file models/distilled-6.0.pt, test_accuracy (0.5200039808917197) > best_test_accuracy(0)\n",
      "00, Saving file models/distilled-6.0.pt, test_accuracy (0.6472929936305732) > best_test_accuracy(0.5200039808917197)\n",
      "01, 02, Saving file models/distilled-6.0.pt, test_accuracy (0.6899880573248408) > best_test_accuracy(0.6472929936305732)\n",
      "03, Saving file models/distilled-6.0.pt, test_accuracy (0.7127786624203821) > best_test_accuracy(0.6899880573248408)\n",
      "04, Saving file models/distilled-6.0.pt, test_accuracy (0.7390525477707006) > best_test_accuracy(0.7127786624203821)\n",
      "05, Saving file models/distilled-6.0.pt, test_accuracy (0.7418391719745223) > best_test_accuracy(0.7390525477707006)\n",
      "06, 07, Saving file models/distilled-6.0.pt, test_accuracy (0.7625398089171974) > best_test_accuracy(0.7418391719745223)\n",
      "08, 09: train: loss 51.390, acc 73.69%, test: loss 148.142, acc 76.07%, best test acc: 76.25%\n",
      "10, 11, Saving file models/distilled-6.0.pt, test_accuracy (0.7670183121019108) > best_test_accuracy(0.7625398089171974)\n",
      "12, 13, 14, Saving file models/distilled-6.0.pt, test_accuracy (0.7894108280254777) > best_test_accuracy(0.7670183121019108)\n",
      "15, 16, Saving file models/distilled-6.0.pt, test_accuracy (0.7924960191082803) > best_test_accuracy(0.7894108280254777)\n",
      "17, 18, Saving file models/distilled-6.0.pt, test_accuracy (0.7943869426751592) > best_test_accuracy(0.7924960191082803)\n",
      "19: train: loss 42.949, acc 78.02%, test: loss 124.314, acc 79.44%, best test acc: 79.44%\n",
      "20, 21, Saving file models/distilled-6.0.pt, test_accuracy (0.7951831210191083) > best_test_accuracy(0.7943869426751592)\n",
      "22, Saving file models/distilled-6.0.pt, test_accuracy (0.7984673566878981) > best_test_accuracy(0.7951831210191083)\n",
      "23, Saving file models/distilled-6.0.pt, test_accuracy (0.8043391719745223) > best_test_accuracy(0.7984673566878981)\n",
      "24, 25, 26, 27, Saving file models/distilled-6.0.pt, test_accuracy (0.807921974522293) > best_test_accuracy(0.8043391719745223)\n",
      "28, 29: train: loss 39.063, acc 80.19%, test: loss 123.615, acc 80.77%, best test acc: 80.79%\n",
      "Saving file models/distilled-6.0.pt, test_accuracy (0.8121019108280255) > best_test_accuracy(0.807921974522293)\n",
      "30, 31, 32, 33, 34, 35, 36, Saving file models/distilled-6.0.pt, test_accuracy (0.820859872611465) > best_test_accuracy(0.8121019108280255)\n",
      "37, 38, 39: train: loss 36.602, acc 81.55%, test: loss 136.381, acc 80.27%, best test acc: 82.09%\n",
      "40, 41, 42, 43, 44, 45, 46, Saving file models/distilled-6.0.pt, test_accuracy (0.8239450636942676) > best_test_accuracy(0.820859872611465)\n",
      "47, 48, 49: train: loss 34.892, acc 82.35%, test: loss 128.542, acc 81.85%, best test acc: 82.39%\n"
     ]
    }
   ],
   "source": [
    "t = 6.0\n",
    "alpha = 0.95\n",
    "\n",
    "dist_model = CNN_Student()\n",
    "best_test_acc  = 0\n",
    "train_loader, test_loader = get_loaders()\n",
    "optimizer = optim.SGD(dist_model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(50):\n",
    "\n",
    "    train_loss, train_acc = train(dist_model, train_loader, optimizer, teacher, t, alpha)\n",
    "    test_loss, test_acc = evaluate(dist_model, test_loader)\n",
    "\n",
    "    if test_acc > best_test_acc:\n",
    "        file_name = os.path.join(MODEL_DIRECTORY, f'distilled-{t}.pt')\n",
    "        print(f'Saving file {file_name}, test_accuracy ({test_acc}) > best_test_accuracy({best_test_acc})')\n",
    "        torch.save(dist_model.state_dict(), file_name)\n",
    "\n",
    "        best_test_acc = test_acc\n",
    "\n",
    "    if epoch % 10 == 9:\n",
    "        print(\n",
    "            f'{epoch:02}: train: loss {train_loss:.3f}, acc {train_acc * 100:.2f}%, ' +\n",
    "                f'test: loss {test_loss:.3f}, acc {test_acc * 100:.2f}%, best test acc: {best_test_acc * 100:.2f}%'\n",
    "        )\n",
    "    else:\n",
    "        print(f'{epoch:02}, ', end='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
